{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "voting_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "environment": {
      "name": "pytorch-gpu.1-6.m59",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSiAYXVfgVdr"
      },
      "source": [
        "# Imports and other\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVf5FX9Ajfb"
      },
      "source": [
        "MODEL_VERSION = \"voting_ensemble\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuj4q2lYTJwP"
      },
      "source": [
        "!pip install catalyst albumentations efficientnet_pytorch torchviz mlconfig fastprogress editdistance\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yJpHKXyd55L"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "from efficientnet_pytorch.utils import get_same_padding_conv2d\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import re \n",
        "import random\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "import pandas as pd\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from CTCDecoder.src.BestPath import ctcBestPath\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from shutil import copyfile\n",
        "from catalyst import dl, metrics\n",
        "from evaluate import evaluate\n",
        "from catalyst.data.cv import ToTensor\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import typing as t\n",
        "from torchviz import make_dot\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import albumentations as A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbdgqObWgiG1"
      },
      "source": [
        "# Preparations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxQExLcJa0Ed"
      },
      "source": [
        "SEED = 42\n",
        "def set_seed(seed: int = 42, set_torch=True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    if set_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False \n",
        "set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0buQn-52glJ5"
      },
      "source": [
        "# Work with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB9mKi39vhxT"
      },
      "source": [
        "MAX_STR_LEN = 71\n",
        "CHARS = [\n",
        "    ' ', '(', ')', '+', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '[', \n",
        "    ']', 'a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', \n",
        "    'p', 'r', 's', 't', 'u', '|', '×', 'ǂ', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', \n",
        "    'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', \n",
        "    'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ѣ', '–', '⊕', '⊗'\n",
        "]\n",
        "CTC_CHARS = CHARS + ['ϵ']  # introduce blank character\n",
        "\n",
        "assert len(CHARS) == len(set(CHARS))\n",
        "\n",
        "N_UNIQUE_CHARS = len(CHARS)\n",
        "BLANK_IDX = N_UNIQUE_CHARS\n",
        "N_UNIQUE_CTC_CHARS = len(CTC_CHARS)\n",
        "\n",
        "def c2idx(char: str) -> int:\n",
        "    assert char in CTC_CHARS, char\n",
        "    return CTC_CHARS.index(char)\n",
        "\n",
        "def idx2c(idx: int):\n",
        "    assert idx < len(CTC_CHARS)\n",
        "    return CTC_CHARS[idx]\n",
        "\n",
        "def dir_pathes2df(image_dir, trans_dir) -> pd.DataFrame:\n",
        "    df = pd.DataFrame({\"imgpath\": sorted(os.listdir(image_dir)), \"textpath\": sorted(os.listdir(trans_dir))})\n",
        "    df[\"imgpath\"] = image_dir + df[\"imgpath\"] \n",
        "    df[\"textpath\"] = trans_dir + df[\"textpath\"] \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHnBp0bSLV9-"
      },
      "source": [
        "IMAGE_SHAPE = (3, 224, 1868)\n",
        "\n",
        "class CommonDataset(Dataset):\n",
        "    def __init__(self, augs):\n",
        "        augs = augs\n",
        "        self.augs = augs\n",
        "        self._transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def transform(self, X) -> torch.FloatTensor:\n",
        "        return self._transform(X).type(torch.FloatTensor)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.imgs) * len(self.augs)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_text(textpath) -> str:\n",
        "        with open(textpath, 'r') as f:\n",
        "            return f.read()\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_img(imgpath) -> np.array:\n",
        "        return cv2.imread(imgpath)\n",
        "        \n",
        "    @staticmethod\n",
        "    def preprocess_text(label) -> str:\n",
        "        return label\n",
        "\n",
        "    @staticmethod\n",
        "    def text_to_labels(text, padlen=MAX_STR_LEN, pad_value=BLANK_IDX, left_pad=False):\n",
        "        '''\n",
        "        swaps characters and indexes, pads with pad_value\n",
        "        PAD_VALUE THEN MUST BE REMOVED TO WORK\n",
        "        '''\n",
        "        labels = np.array(list(map(lambda x: c2idx(x), text)))\n",
        "        l = len(labels)\n",
        "        assert l <= padlen\n",
        "        padding = np.full(padlen - l, pad_value)\n",
        "        if left_pad:\n",
        "            return np.concatenate((padding, labels))\n",
        "        else:\n",
        "            return np.concatenate((labels, padding))\n",
        "\n",
        "    def whole_image_pipeline(self, img):\n",
        "        return self.transform(self.preprocess_img(img))\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess_img(img):\n",
        "        w, h,_ = img.shape\n",
        "        if w > 2.5 * h:\n",
        "            img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "            w, h = h, w\n",
        "        new_w = IMAGE_SHAPE[1]\n",
        "        new_h = int(h * (new_w / w))\n",
        "        img = cv2.resize(img, (new_h, new_w))\n",
        "        w, h,_ = img.shape\n",
        "        img = img.astype('float32')\n",
        "        if w < new_w:\n",
        "            add_zeros = np.full((new_w - w, h, 3), 255)\n",
        "            img = np.concatenate((img, add_zeros))\n",
        "            w, h,_ = img.shape\n",
        "        if h < IMAGE_SHAPE[2]:\n",
        "            add_zeros = np.full((w, IMAGE_SHAPE[2] - h,3), 255)\n",
        "            img = np.concatenate((img, add_zeros), axis=1)\n",
        "            w, h,_ = img.shape\n",
        "\n",
        "        if h > IMAGE_SHAPE[2] or w > new_w:\n",
        "            dim = (IMAGE_SHAPE[2], IMAGE_SHAPE[1])\n",
        "            img = cv2.resize(img, dim)\n",
        "\n",
        "        img = cv2.subtract(255, img)\n",
        "        img = img / 255\n",
        "        return img\n",
        "    \n",
        "    def __getitem__(self, i) -> dict:\n",
        "        aug_idx = i // len(self.imgs)\n",
        "        i %= len(self.imgs)\n",
        "        \n",
        "        img, text = self.get_item(i)\n",
        "        \n",
        "        img = self.augs[aug_idx](image=img)[\"image\"]\n",
        "\n",
        "        img = self.preprocess_img(img)\n",
        "        label = self.text_to_labels(self.preprocess_text(text))\n",
        "\n",
        "        return {\n",
        "            'image': self.transform(img),\n",
        "            'label': torch.from_numpy(label).type(torch.LongTensor),\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "class LazyDataset(CommonDataset):\n",
        "    def __init__(self, df, augs=[]):\n",
        "        super(LazyDataset, self).__init__(augs)\n",
        "        self.imgs = df[\"imgpath\"].values\n",
        "        self.texts = df[\"textpath\"].values\n",
        "        logging.info(f'Creating dataset with {len(self.imgs)} examples')    \n",
        "\n",
        "    def get_item(self, i):\n",
        "        # returns raw image and text\n",
        "        text_file = self.texts[i]\n",
        "        img_file = self.imgs[i]\n",
        "        text = self.read_text(text_file)\n",
        "        img = self._read_img(img_file)\n",
        "        return img, text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUbZTnS7x9wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac720dc5-b02e-4dfb-e2e4-f02bb68890a9"
      },
      "source": [
        "if not os.path.exists(\"/data\"):\n",
        "    from checker import checker\n",
        "    !sudo mkdir /data\n",
        "    !sudo wget https://storage.yandexcloud.net/datasouls-ods/materials/46b7bb85/datasets.zip -P /data\n",
        "    !sudo unzip /data/datasets.zip -d /data/\n",
        "    !sudo rm /data/datasets.zip\n",
        "    num_corr, num = checker(\"/data/train/words\")\n",
        "    cv2.imwrite(\"/data/train/images/221_10_23.jpg\", cv2.rotate(cv2.imread(\"/data/train/images/221_10_23.jpg\"), cv2.ROTATE_90_CLOCKWISE))\n",
        "    !printf 'вашему величеству лицеземной поклон' > /data/train/words/197_17_2.txt\n",
        "    !printf 'ной прямую накрестъ линѣю как видима вцев' > /data/train/words/217_40_13.txt\n",
        "    !printf '[около сих мѣстъ [на половинѣ' > /data/train/words/343_46_35.txt\n",
        "    !printf 'явитца адмира' > /data/train/words/368_3_0.txt\n",
        "    !printf '+ того же смотрѣт i въ воiнской амунициi' > /data/train/words/416_2_12.txt\n",
        "    !printf 'i потом немедленно пришлите' > /data/train/words/47_27_5.txt\n",
        "    !printf 'тотчас сие по воли' > /data/train/words/77_28_14.txt\n",
        "    clear_output()\n",
        "    print('\\nSTATISTICS')\n",
        "    print('Number of corrected files = ' + str(num_corr))\n",
        "    print('Total number of files = ' + str(num))\n",
        "    print('Percentage of corrected files = ' + str(np.round(num_corr/num * 100, 2)) + '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "STATISTICS\n",
            "Number of corrected files = 81\n",
            "Total number of files = 6196\n",
            "Percentage of corrected files = 1.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5eAZMN5o4nK"
      },
      "source": [
        "image_dir = '/data/train/images/'\n",
        "trans_dir = '/data/train/words/'\n",
        "\n",
        "df = dir_pathes2df(image_dir, trans_dir)\n",
        "def read_text(path):\n",
        "    with open(path, 'r') as file:\n",
        "        return file.read()\n",
        "df[\"text\"] = df[\"textpath\"].apply(read_text)\n",
        "\n",
        "contains_del_chars = lambda x: bool(set(x) - set(CHARS))\n",
        "train_df, test_df = train_test_split(df, test_size=0.05, random_state=SEED)\n",
        "train_df = train_df[~train_df[\"text\"].apply(contains_del_chars)]\n",
        " \n",
        "whole_dataset = LazyDataset(df, [A.NoOp()])\n",
        "train_dataset = LazyDataset(train_df, [A.NoOp()])\n",
        "test_dataset = LazyDataset(test_df, [A.NoOp()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nht7FyZp7oZO"
      },
      "source": [
        "# Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvzb3pKrnTrB"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baOJalYzXyb3"
      },
      "source": [
        "def get_output_shape(model, input_shape):\n",
        "    input_shape = (1, ) + input_shape\n",
        "    with torch.no_grad():\n",
        "        return model(torch.rand(*input_shape)).shape[1:]\n",
        "\n",
        "\n",
        "class EfficientNetFe(EfficientNet):\n",
        "    def _stem_and_blocks_output(self, inputs):\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks) # scale drop connect_rate\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "        return x\n",
        "\n",
        "    def get_blocks_shape(self, image_shape=IMAGE_SHAPE):\n",
        "        return get_output_shape(self._stem_and_blocks_output, image_shape)\n",
        "\n",
        "    def drop_last_n_layers(self, n):\n",
        "        # specifies how much layers to drop from feature extractor\n",
        "        if n != 0:\n",
        "            self._blocks = self._blocks[:-n]\n",
        "        new_output_shape = self.get_blocks_shape()\n",
        "        self._conv_head = get_same_padding_conv2d(image_size=new_output_shape[1:])(\n",
        "            new_output_shape[0], new_output_shape[0] * 4, self._conv_head.kernel_size\n",
        "        )\n",
        "        self._bn1 = nn.BatchNorm2d(new_output_shape[0] * 4)\n",
        "        print(\"Output shape\", get_output_shape(self, IMAGE_SHAPE))\n",
        "        return self\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Stem and Blocks\n",
        "        x = self._stem_and_blocks_output(inputs)\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6iS8aUr7qDh"
      },
      "source": [
        "class OCRBackboneRnnModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(OCRBackboneRnnModel, self).__init__()\n",
        "        # backbone\n",
        "        self.model_name = config[\"model_name\"]\n",
        "        self.backbone = config[\"backbone\"]\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = config[\"train_backbone\"]\n",
        "\n",
        "        # lstm \n",
        "        self.pooling = nn.AdaptiveMaxPool2d((1, None))\n",
        "        rnn_hidden_size = config[\"rnn_hidden_size\"]\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=config[\"backbone_output_shape\"][0], hidden_size=rnn_hidden_size, num_layers=2, \n",
        "            bidirectional=True, dropout=config[\"rnn_dropout\"]\n",
        "        )\n",
        "        self.char_classifier = nn.Linear(rnn_hidden_size*2, N_UNIQUE_CTC_CHARS)\n",
        "\n",
        "        self.ctc_decode = lambda mat: ctcBestPath(mat, CHARS)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "        print(\"N parameters\", self.count_parameters())\n",
        "\n",
        "    def forward(self, X):\n",
        "        assert X.shape[1] in (1, 3), \"Images must have 1 or 3 channels\"\n",
        "        features = self.backbone(X)\n",
        "        timesteps = self.pooling(features).squeeze(2).permute(2, 0, 1) # [b, c, h, w] -> [w, b, c]\n",
        "        rnn_output, h_n = self.rnn(timesteps)\n",
        "        cla = self.char_classifier(rnn_output)\n",
        "        return cla\n",
        "\n",
        "    def _preds_to_probs(self, logits):\n",
        "        # logits shape: [t, b, c]\n",
        "        y_hat_for_decode = logits.transpose(0, 1)  # transpose makes: [t, b, c] -> [b, t, c]\n",
        "        y_hat_for_decode = self.softmax(y_hat_for_decode)\n",
        "        y_hat_for_decode = y_hat_for_decode.cpu().numpy()\n",
        "        return y_hat_for_decode\n",
        "\n",
        "    def predict(self, X):\n",
        "        with torch.no_grad():\n",
        "            y_hat = self.forward(torch.Tensor(X).to(device))\n",
        "        \n",
        "        y_hat_for_decode = self._preds_to_probs(y_hat)\n",
        "        texts = [self.ctc_decode(mat) for mat in y_hat_for_decode] \n",
        "        return {\"texts\": texts, \"raw\": y_hat}\n",
        "\n",
        "    def predict_dataset(self, X, bs=32):\n",
        "        preds = []\n",
        "        raws = []   \n",
        "        for i in range(math.ceil(len(X) / bs)):\n",
        "            p = self.predict(X[i*bs:(i+1)*bs])\n",
        "            preds.extend(p[\"texts\"])\n",
        "            raws.extend([\"raw\"])\n",
        "        return {\"texts\": preds, \"raw\": raws}\n",
        "\n",
        "    def train(self, mode=True, train_bn=True):\n",
        "        super(OCRBackboneRnnModel, self).train(mode)\n",
        "        if not train_bn:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.BatchNorm2d):\n",
        "                    m.weight.requires_grad_(False)\n",
        "                    m.bias.requires_grad_(False)\n",
        "                    m.eval()\n",
        "\n",
        "    def count_parameters(self, all=False):\n",
        "        return sum(p.numel() for p in self.parameters() if (True if all else p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ct8E9WVLrUI"
      },
      "source": [
        "def get_inference_config(backbone_name, rnn_hidden_size, rnn_dropout):\n",
        "    n_to_drop = {\n",
        "        \"efficientnet-b4\": 10,\n",
        "        \"efficientnet-b5\": 12,\n",
        "        \"efficientnet-b6\": 14,\n",
        "        \"efficientnet-b7\": 17,\n",
        "    }\n",
        "    backbone = EfficientNetFe.from_name(backbone_name)\n",
        "    backbone.drop_last_n_layers(n_to_drop[backbone_name])\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"backbone_output_shape\": get_output_shape(backbone, IMAGE_SHAPE),\n",
        "        \"train_backbone\": False,\n",
        "        \"model_name\": \"ocr_model\",\n",
        "        \"rnn_hidden_size\": rnn_hidden_size,\n",
        "        \"rnn_dropout\": rnn_dropout\n",
        "    }\n",
        "\n",
        "\n",
        "class VotingEnsemble:\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for model in self.models:\n",
        "            preds.append(model.predict(X)[\"texts\"])\n",
        "        preds = np.array(preds)\n",
        "        preds = stats.mode(preds, axis=0)[0].squeeze().tolist()\n",
        "        return preds\n",
        "\n",
        "    def predict_dataset(self, X, bs=32):\n",
        "        preds = []\n",
        "        for i in range(math.ceil(len(X) / bs)):\n",
        "            p = self.predict(X[i*bs:(i+1)*bs])\n",
        "            preds.extend(p)\n",
        "        return {\"texts\": preds}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_ftvkd4VLWq"
      },
      "source": [
        "## Check that model is all right on pseudo test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSZq5wTdsjTs"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "\n",
        "whole_loader = DataLoader(whole_dataset, BATCH_SIZE, True)\n",
        "train_loader = DataLoader(train_dataset, BATCH_SIZE, True)\n",
        "test_loader = DataLoader(test_dataset, BATCH_SIZE, False)\n",
        "\n",
        "loaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"valid\": test_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaq-UZ0uKefM"
      },
      "source": [
        "# First B5\n",
        "first_omega_config = get_inference_config(\"efficientnet-b5\", 256, 0.2)\n",
        "first_omega_model = OCRBackboneRnnModel(first_omega_config).to(device)\n",
        "first_omega_path = f\"pretrained_models/post_best_full_vfirst_omega.pth\"\n",
        "first_omega_pretrained = torch.load(first_omega_path)\n",
        "first_omega_model.load_state_dict(first_omega_pretrained[\"model_state_dict\"])\n",
        "first_omega_model.train(False, False);\n",
        "# Large B5\n",
        "omega_large_config = get_inference_config(\"efficientnet-b5\", 256, 0.2)\n",
        "omega_large_model = OCRBackboneRnnModel(omega_large_config).to(device)\n",
        "omega_large_path = f\"pretrained_models/post_best_full_vomega_large.pth\"\n",
        "omega_large_pretrained = torch.load(omega_large_path)\n",
        "omega_large_model.load_state_dict(omega_large_pretrained[\"model_state_dict\"])\n",
        "omega_large_model.train(False, False);\n",
        "# First B6 \n",
        "theta_first_config = get_inference_config(\"efficientnet-b6\", 256, 0.2)\n",
        "theta_first_model = OCRBackboneRnnModel(theta_first_config).to(device)\n",
        "theta_first_path = f\"pretrained_models/post_best_full_vtheta_first.pth\"\n",
        "theta_first_pretrained = torch.load(theta_first_path)\n",
        "theta_first_model.load_state_dict(theta_first_pretrained[\"model_state_dict\"])\n",
        "theta_first_model.train(False, False);\n",
        "# Second B6 \n",
        "theta_second_config = get_inference_config(\"efficientnet-b6\", 256, 0.2)\n",
        "theta_second_model = OCRBackboneRnnModel(theta_second_config).to(device)\n",
        "theta_second_path = f\"pretrained_models/post_best_full_vtheta_second.pth\"\n",
        "theta_second_pretrained = torch.load(theta_second_path)\n",
        "theta_second_model.load_state_dict(theta_second_pretrained[\"model_state_dict\"])\n",
        "theta_second_model.train(False, False);\n",
        "# Third B6\n",
        "theta_third_config = get_inference_config(\"efficientnet-b6\", 256, 0.2)\n",
        "theta_third_model = OCRBackboneRnnModel(theta_third_config).to(device)\n",
        "theta_third_path = f\"pretrained_models/post_best_full_vtheta_third.pth\"\n",
        "theta_third_pretrained = torch.load(theta_third_path)\n",
        "theta_third_model.load_state_dict(theta_third_pretrained[\"model_state_dict\"])\n",
        "theta_third_model.train(False, False);\n",
        "# First B7\n",
        "beta_config = get_inference_config(\"efficientnet-b7\", 256, 0.2)\n",
        "beta_model = OCRBackboneRnnModel(beta_config).to(device)\n",
        "beta_path = f\"pretrained_models/post_best_full_vbeta.pth\"\n",
        "beta_pretrained = torch.load(beta_path)\n",
        "beta_model.load_state_dict(beta_pretrained[\"model_state_dict\"])\n",
        "beta_model.train(False, False);\n",
        "\n",
        "models = [\n",
        "    first_omega_model, omega_large_model, theta_first_model, \n",
        "    theta_second_model, theta_third_model, beta_model\n",
        "]\n",
        "model = VotingEnsemble(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nUEH4oHtnp7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f0d6c339-7b66-4b07-ffc7-735c03b6185c"
      },
      "source": [
        "preds = []\n",
        "true = []\n",
        "for b in progress_bar(iter(test_loader)):\n",
        "    preds.extend(model.predict_dataset(b[\"image\"], BATCH_SIZE)[\"texts\"])\n",
        "    true.extend(b[\"text\"])\n",
        "print(evaluate(true, preds, 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='155' class='' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [155/155 00:31<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Character error rate: 0.118554%\n",
            "Word error rate: 0.740284%\n",
            "String accuracy: 97.419355%\n",
            "{'cer': 0.11855364552459988, 'wer': 0.7402837754472548, 'sa': 97.41935483870968}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}